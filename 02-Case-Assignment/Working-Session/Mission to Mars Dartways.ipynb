{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "import pymongo\n",
    "import requests\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Current google-chrome version is 87.0.4280\n",
      "[WDM] - Get LATEST driver version for 87.0.4280\n",
      "[WDM] - Driver [/Users/fraolabebe/.wdm/drivers/chromedriver/mac64/87.0.4280.88/chromedriver] found in cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    }
   ],
   "source": [
    "# Configure ChromeDriver\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NASA Mars News\n",
    "\n",
    "* Scrape the [NASA Mars News Site](https://mars.nasa.gov/news/) and collect the latest News Title and Paragraph Text. Assign the text to variables that you can reference later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mars_news():\n",
    "    url = 'https://mars.nasa.gov/news/'\n",
    "    browser.visit(url)\n",
    "\n",
    "    html = browser.html\n",
    "    news_soup = Soup(html, 'html.parser')\n",
    "\n",
    "    news_soup.find('ul', class_='item_list')\n",
    "\n",
    "    headline_date = article_container.find('div', class_='list_date').text\n",
    "    title = article_container.find('div', class_='content_title').find('a').text\n",
    "    paragraph = article_container.find('div', class_='article_teaser_body').text\n",
    "\n",
    "    return title, paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JPL Mars Space Images - Featured Image\n",
    "\n",
    "* Visit the url for JPL Featured Space Image [here](https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars).\n",
    "\n",
    "* Use splinter to navigate the site and find the image url for the current Featured Mars Image and assign the url string to a variable called `featured_image_url`.\n",
    "\n",
    "* Make sure to find the image url to the full size `.jpg` image.\n",
    "\n",
    "* Make sure to save a complete url string for this image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featured_image():\n",
    "    base_url = 'https://www.jpl.nasa.gov'\n",
    "\n",
    "    url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "    browser.visit(url)\n",
    "\n",
    "    html = browser.html\n",
    "    img_soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Method 1: parsing through the style attribute in the article tag\n",
    "    try:\n",
    "        img_elem = img_soup.find('article', class_='carousel_item')['style']\n",
    "        img_rel_url = img_elem.replace(\"background-image: url('\", '')\n",
    "        img_rel_url = img_rel_url.replace(\"');\", '')\n",
    "        print(img_rel_url)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Method 2: clicking the FULL TEXT button and pulling the image\n",
    "    try:\n",
    "        full_image_elem = browser.find_by_id('full_image')[0]\n",
    "        full_image_elem.click()\n",
    "\n",
    "        html = browser.html\n",
    "        img_soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        img_rel_url = img_soup.find('img', class_='fancybox-image')['src']\n",
    "        print(img_rel_url)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    featured_image_url  = f'{base_url}{img_rel_url}'\n",
    "    print(featured_image_url)\n",
    "    \n",
    "    return featured_image_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mars Facts \n",
    "webpage [here](https://space-facts.com/mars/) and use Pandas to scrape the table containing facts about the planet including Diameter, Mass, etc.\n",
    "\n",
    "Use Pandas to convert the data to a HTML table string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mars_facts():\n",
    "    url = 'https://space-facts.com/mars/'\n",
    "    browser.visit(url)\n",
    "\n",
    "    mars_facts_df = pd.read_html(url)\n",
    "    mars_facts_df = mars_facts_df[0]\n",
    "    mars_facts_df.columns = ['Description', 'Mars']\n",
    "    mars_facts_df\n",
    "\n",
    "    mars_facts_html = mars_facts_df.to_html(classes='table table-striped')\n",
    "    \n",
    "    return mars_facts_html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mars Hemispheres\n",
    "\n",
    "* Visit the USGS Astrogeology site [here](https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars) to obtain high resolution images for each of Mar's hemispheres.\n",
    "\n",
    "* You will need to click each of the links to the hemispheres in order to find the image url to the full resolution image.\n",
    "\n",
    "* Save both the image url string for the full resolution hemisphere image, and the Hemisphere title containing the hemisphere name. Use a Python dictionary to store the data using the keys `img_url` and `title`.\n",
    "\n",
    "* Append the dictionary with the image url string and the hemisphere title to a list. This list will contain one dictionary for each hemisphere.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-2d3eea46f26a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_url\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbrowser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbrowser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulsoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "base_url = 'https://astrogeology.usgs.gov'\n",
    "url = base_url + '/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "browser.visit(url)\n",
    "time.sleep(1)\n",
    "html = browser.html\n",
    "soup = Beautifulsoup(html, 'html.parser')\n",
    "items = soup.find_all('div', class_='item')\n",
    "urls = []\n",
    "titles = []\n",
    "for item in items:\n",
    "    urls.append(base_url + item.find('a')['href'])\n",
    "    titles.append(item.find('h3').text.strip())\n",
    "img_urls = []\n",
    "for oneurl in urls:\n",
    "    browser.visit(oneurl)\n",
    "    time.sleep(1)\n",
    "    html = browser.html\n",
    "    soup = bs(html, 'html.parser')\n",
    "    oneurl = base_url+soup.find('img',class_='wide-image')['src']\n",
    "    img_urls.append(oneurl)\n",
    "hemisphere_image_urls = []\n",
    "for i in range(len(titles)):\n",
    "    hemisphere_image_urls.append({'title':titles[i],'img_url':img_urls[i]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
